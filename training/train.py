import typingimport warningsimport torchfrom datasets.base import DataLoaderimport datasets.registryfrom foundations import hparamsfrom foundations import pathsfrom foundations.step import Stepfrom models.base import Modelimport models.registryfrom platforms.platform import get_platformfrom training.checkpointing import restore_checkpointfrom training import optimizersfrom training import standard_callbacksfrom training.metric_logger import MetricLoggerfrom attacks.adv_train_util import get_attackfrom utilities.capacity_utils import get_classifier_constant, get_feature_regfrom utilities.evaluation_utils import report_advdef train(        dataset_hparams: hparams.DatasetHparams,        training_hparams: hparams.TrainingHparams,        model: Model,        train_loader: DataLoader,        output_location: str,        callbacks: typing.List[typing.Callable] = [],        adv_attack: typing.Callable = None,        start_step: Step = None,        end_step: Step = None,        dataset_name : str = 'mnist'):        """The main training loop for this framework.         Args:      * dataset_hparams: The dataset hyperparameters whose schema is specified in hparams.py.      * training_hparams: The training hyperparameters whose schema is specified in hparams.py.      * model: The model to train. Must be a models.base.Model      * train_loader: The training data. Must be a datasets.base.DataLoader      * output_location: The string path where all outputs should be stored.      * callbacks: A list of functions that are called before each training step and once more        after the last training step. Each function takes five arguments: the current step,        the output location, the model, the optimizer, and the logger.        Callbacks are used for running the test set, saving the logger, saving the state of the        model, etc. They provide hooks into the training loop for customization so that the        training loop itself can remain simple.      * adv_attack: A function that carries out an adversarial attack on a batch of inputs.      * start_step: The step at which the training data and learning rate schedule should begin.        Defaults to step 0.      * end_step: The step at which training should cease. Otherwise, training will go for the        full `training_hparams.training_steps` steps.    """    # create the output location if it doesn't already exist.    if not get_platform().exists(output_location) and get_platform().is_primary_process:        get_platform().makedirs(output_location)        # get the optimizer and learning rate schedule.    model.to(get_platform().torch_device)    optimizer = optimizers.get_optimizer(training_hparams, model)    step_optimizer = optimizer    lr_schedule = optimizers.get_lr_schedule(training_hparams, optimizer, train_loader.iterations_per_epoch)    eta = 0.1  # regularization constant for orthogonal layer weights    # Get the random seed for the data order.    data_order_seed = training_hparams.data_order_seed        # Restore the model from a saved checkpoint if the checkpoint exists    cp_step, cp_logger = restore_checkpoint(output_location, model, optimizer, train_loader.iterations_per_epoch)    start_step = cp_step or start_step or Step.zero(train_loader.iterations_per_epoch)    logger = cp_logger or MetricLogger()    with warnings.catch_warnings():        warnings.filterwarnings("ignore", category=UserWarning)        for _ in range(start_step.iteration):            lr_schedule.step()        # Determine when to end training    end_step = end_step or Step.from_str(training_hparams.training_steps, train_loader.iterations_per_epoch)    if end_step <= start_step:        return        # The training loop.    for epoch in range(start_step.ep, end_step.ep + 1):        # Ensure the data order is different for each epoch.        train_loader.shuffle(None if data_order_seed is None else (data_order_seed + epoch))                for iteration, (examples, labels) in enumerate(train_loader):                        # Advance the data loader until the start epoch and iteration            if epoch == start_step.ep and iteration < start_step.it:                continue                        # Run the call backs            step = Step.from_epoch(epoch, iteration, train_loader.iterations_per_epoch)            for callback in callbacks:                callback(output_location, step, model, optimizer, logger)                        # Exit at the end step            if epoch == end_step.ep and iteration == end_step.it:                return                        # Train!            examples = examples.to(device=get_platform().torch_device)            labels = labels.to(device=get_platform().torch_device)            labels_size = torch.tensor(len(labels), device=get_platform().torch_device)            # print('batch size initially is {}'.format(labels_size))            if dataset_hparams.gaussian_augment:                augment_examples = torch.zeros_like(examples).to(device=get_platform().torch_device)                augment_examples.normal_(dataset_hparams.gaussian_aug_mean, dataset_hparams.gaussian_aug_std)                examples = torch.cat((examples, augment_examples), dim=0)                labels = torch.cat((labels, labels), dim=0)            delta = torch.zeros_like(examples).to(device=get_platform().torch_device)            if adv_attack and training_hparams.adv_train:                delta = adv_attack(model, examples, labels, training_hparams.adv_train_attack_power, training_hparams.adv_train_attack_power/10, training_hparams.adv_train_attack_iter)                # lets do some journalism to see if its faithful. and it works!                if (epoch % 2 == 0) and (iteration == 0):                    report_adv(model, examples, labels, delta)                examples = torch.cat((examples, examples+delta), dim=0)                labels = torch.cat((labels, labels), dim=0)                # labels_size = torch.tensor(len(labels), device=get_platform().torch_device)                # print('batch size after adversarial training is {}'.format(labels_size))            step_optimizer.zero_grad()            model.train()            loss_classification = model.loss_criterion(model(examples), labels)            if model.model_name.startswith('mnist_lenet'):                loss_ortho, loss_euclidean, num_layers = get_feature_reg(model, epoch, iteration, True, True)                loss_cw = get_classifier_constant(model, labels, False)                if (epoch == 0) and (iteration == 0):                    labels_size = torch.tensor(len(labels), device=get_platform().torch_device)                    print("size of augmented data set is {}".format(labels_size))                if iteration == 0:                    print("Classification loss is {}".format(1.0 * loss_classification))                    print("Regularization loss is {}".format(1.0/num_layers * loss_ortho + 1.0/num_layers * loss_euclidean))                    print("Classifier constant regularizer is {}".format(1.0 * loss_cw))                # replicate 8 : 3.5, 0.0, 0.0, 0.0                # replicate 9 : 3.5, 0.3, 2.1, 0.0                # replicate 10 : 3.5, 0.0, 0.0, 0.0                # replicate 11 : 1.0, 0.0, 0.0, 0.0                # replicate 18 : 1.0, 0.0, 0.0, 0.0 - no regularization. no adversarial training.                # replicate 19 : 1.0, 0.0, 0.0, 0.0 - no regularization, yes adversarial training.                # replicate 20 : 3.5, 0.3, 2.1, 0.0 - ortho fram reg + major euclidean row norm reg, no adv training                # replicate 21 : 3.5, 0.3, 2.1, 0.0 - yes regularization, yes adversarial training.                loss = 1.0 * loss_classification + 0.0/num_layers * loss_ortho + 0.0/num_layers * loss_euclidean + 0.0 * loss_cw                loss.backward()            else:                loss = loss_classification                loss.backward()            # Step forward. Ignore warnings that the lr_schedule generates.            step_optimizer.step()            with warnings.catch_warnings():                warnings.filterwarnings("ignore", category=UserWarning)                lr_schedule.step()def standard_train(        model: Model,        output_location: str,        dataset_hparams: hparams.DatasetHparams,        training_hparams: hparams.TrainingHparams,        start_step: Step = None,        verbose: bool = True,        evaluate_every_epoch: bool = True):    """ Train using the standard callbacks according to the provided hparams."""        # If the model file for the end of training already exists in this location, do not train    iterations_per_epoch = datasets.registry.iterations_per_epoch(dataset_hparams)    train_end_step = Step.from_str(training_hparams.training_steps, iterations_per_epoch)    penultimate_ep = '38ep'    train_penultimate_step = Step.from_str(penultimate_ep, iterations_per_epoch)    train_loader = datasets.registry.get(dataset_hparams, train=True)    test_loader = datasets.registry.get(dataset_hparams, train=False)    name = dataset_hparams.dataset_name    print(model.model_name)    print("Sanity check, dataset name is : " + str(name))    print("Printing model too :")    print(model)    adv_attack = get_attack(training_hparams)    callbacks = standard_callbacks.standard_callbacks(        training_hparams,        train_loader,        test_loader,        start_step=start_step,        verbose=verbose,        evaluate_every_epoch=evaluate_every_epoch)    if models.registry.exists(output_location, train_end_step) and get_platform().exists(paths.logger(output_location)):        train(dataset_hparams, training_hparams, model, train_loader, output_location, callbacks, adv_attack, start_step=train_penultimate_step, dataset_name=name)    else:        train(dataset_hparams, training_hparams, model, train_loader, output_location, callbacks, adv_attack, start_step=start_step, dataset_name=name)